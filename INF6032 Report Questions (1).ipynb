{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a73380f-c2d9-4eb0-80fa-5f5f1d58d174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Startup Requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e987b8fd-132e-4aed-9955-590e825b10f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import pyspark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218937a2-f21c-4eba-8d75-6a1ac1a30a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#all functions that need to be imported for the code to properly run\n",
    "from pyspark.ml.feature import NGram, StopWordsRemover\n",
    "from pyspark.sql.functions import col, concat_ws, count, desc, explode, lower, regexp_replace\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1884fb25-a0c5-47c4-894c-a331fc06c46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016ef673-b106-494e-8dfe-2161ae8c2459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Uploading the data files\n",
    "large = spark.read.format ('csv') \\\n",
    "    .option(\"header\", \"True\") \\\n",
    "        .load(\"dbfs:/FileStore/shared_uploads/bdavies5@sheffield.ac.uk/large.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387a9f05-c933-41c3-a80c-e412a86909c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 1 - Calculate the number of different sentences in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8027f9d2-eb2d-43a4-9f88-27111ceba16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#counting the number of distinct rows in the large dataset\n",
    "No_sentences_L = large.select('sentence').distinct().count()\n",
    "\n",
    "#outputing the value collected\n",
    "print(f\"\\nThe number of sentences in the Large dataset is: { No_sentences_L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc5709b-b0cf-446a-a765-142aa5174429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#further exploration\n",
    "\n",
    "#counting the number of rows in the large dataset\n",
    "No_sentences_L = large.select('sentence').count()\n",
    "\n",
    "#outputing the value collected\n",
    "print(f\"\\nThe number of duplicate sentences in the Large dataset is: { No_sentences_L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7099207-d23b-4608-a357-e22a7845db48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing the required functions for finding most common duplicate\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "#group by sentences, so duplciates will be grouped together, and count the number of duplciates\n",
    "sentence_grouped = large.groupBy(\"sentence\").count()\n",
    "\n",
    "#filter the dataset so that only the duplciates remain\n",
    "duplicate_sentences = sentence_grouped.filter(col(\"count\") > 1)\n",
    "\n",
    "#sort by most common to least common duplicate\n",
    "most_common_duplicate = duplicate_sentences.orderBy(desc(\"count\")).limit(1)\n",
    "\n",
    "#output the results\n",
    "most_common_duplicate.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e320215-bdcf-404f-a0fb-eab08310683d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 2 - List the numbers of words in the 10 longest sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c80674c-8b90-45f9-ad77-7b78fbfb99e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#split sentences into individual words in an array and count the words in each sentence\n",
    "large_array = large.withColumn(\"sentence\", F.split(F.col(\"sentence\"), \" \")) \\\n",
    "                   .withColumn(\"large_word_count\", F.size(\"sentence\"))\n",
    "\n",
    "#sort by the word count in descending order, largest to smallest\n",
    "top_10_sentences_L = large_array.orderBy(F.desc(\"large_word_count\"))\n",
    "\n",
    "#Output top 10 sentences\n",
    "top_10_sentences_L.select(\"large_word_count\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b3ffe9-fc3a-45ba-9cfc-dd5ea198f199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Further Investigation\n",
    "\n",
    "#printing out the longest sentencez in full\n",
    "row = top_10_sentences_L.select(\"sentence\").first()\n",
    "sentence_array = row[\"sentence\"]\n",
    "sentence_clean = \" \".join(word.strip('\"') for word in sentence_array)\n",
    "print(sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca41111f-9a1a-4103-ac1b-0fb5d9ac914f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 3 - The average number of bigrams per sentence across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b51e768-51a1-4e23-b120-d7840932bcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Answer to question three using large dataset\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import count, col, explode\n",
    "\n",
    "#remove punctuation from the dataset\n",
    "large_no_punctuation = large.withColumn(\"sentence\", regexp_replace(\"sentence\", r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "#seperate out words into individual tokens\n",
    "large_array = large_no_punctuation.withColumn(\"sentence\", F.split(col(\"sentence\"), \" \"))\n",
    "\n",
    "\n",
    "#create the bigrams\n",
    "ngram = NGram(n=2, inputCol=\"sentence\", outputCol=\"bigrams\")\n",
    "large_bigrams = ngram.transform(large_array)\n",
    "\n",
    "#explode bigram column\n",
    "large_exploded = large_bigrams.select(explode(col(\"bigrams\")))\n",
    "\n",
    "\n",
    "#find the average\n",
    "No_sentences_L = large.select('sentence').count()\n",
    "no_bigrams_L = large_exploded.select('col').count()\n",
    "\n",
    "total_L = no_bigrams_L/No_sentences_L\n",
    "\n",
    "#output\n",
    "print(f\"Average bigrims in large dataset is: {total_L}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9524a578-7bf8-4721-9827-f6bfed02fc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#futher exploration - avergae number of words per sentence\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "#explode the tokenised word column\n",
    "word_exploded = large_array.select((explode(\"sentence\")).alias('sentence'))\n",
    "\n",
    "#count number of words\n",
    "no_words = word_exploded.select('sentence').count()\n",
    "\n",
    "#find average and output\n",
    "total_w = no_words/No_sentences_L\n",
    "print(f\"The averge number of words per sentence is: {total_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c37660-a386-4998-903e-462e71180900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 4 - The 10 most frequent bigrams in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67eec5de-8e06-4ed9-b840-fa6e5626c45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#functions imported\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.functions import concat_ws, count, explode, col, lower\n",
    "\n",
    "#removal of punctuation\n",
    "large_no_punctuation = large.withColumn(\"sentence\", regexp_replace(\"sentence\", r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "#case normalisation\n",
    "large_lower = large_no_punctuation.withColumn(\"sentence_lower\", lower(col(\"sentence\")))\n",
    "\n",
    "#tokenisation\n",
    "large_array = large_lower.withColumn(\"sentence\", F.split(F.col(\"sentence_lower\"), \" \"))\n",
    "\n",
    "#create the bigrams\n",
    "ngram = NGram(n=2, inputCol=\"sentence\", outputCol=\"bigrams\")\n",
    "large_bigrams = ngram.transform(large_array)\n",
    "\n",
    "\n",
    "#make each bigram a row\n",
    "large_exploded = large_bigrams.select(explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "\n",
    "#convert the bigrams from an array to string to count\n",
    "large_exploded_str = large_exploded.withColumn(\"bigram_str\", concat_ws(\" \", col(\"bigram\")))\n",
    "\n",
    "#count most frequent bigrams\n",
    "large_count = large_exploded_str.groupBy(\"bigram_str\").agg(count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\"))\n",
    "\n",
    "\n",
    "#output\n",
    "large_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0257a6d-76fc-4ee0-af2a-ef4d577b03c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#further investigaion, removal of stopwords\n",
    "\n",
    "#functions imported\n",
    "from pyspark.ml.feature import NGram, StopWordsRemover\n",
    "from pyspark.sql.functions import concat_ws, count, explode, col, lower\n",
    "\n",
    "#removal of punctuation\n",
    "large_no_punctuation = large.withColumn(\"sentence\", regexp_replace(\"sentence\", r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "#case normalisation\n",
    "large_lower = large_no_punctuation.withColumn(\"sentence_lower\", lower(col(\"sentence\")))\n",
    "\n",
    "#tokenisation\n",
    "large_array = large_lower.withColumn(\"sentence\", F.split(F.col(\"sentence_lower\"), \" \"))\n",
    "\n",
    "#removal of stopwords\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"sentence_filtered\")\n",
    "large_array_removed = remover.transform(large_array)\n",
    "\n",
    "#create the bigrams\n",
    "ngram = NGram(n=2, inputCol=\"sentence_filtered\", outputCol=\"bigrams\")\n",
    "large_bigrams = ngram.transform(large_array_removed)\n",
    "\n",
    "\n",
    "#make each bigram a row\n",
    "large_exploded = large_bigrams.select(explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "\n",
    "#convert the bigrams from an array to string to count\n",
    "bigrams_str = large_exploded.withColumn(\"bigram_str\", concat_ws(\" \", col(\"bigram\")))\n",
    "\n",
    "#remove empty rows\n",
    "cleaned_bigrams_str = bigrams_str.filter(F.trim(col(\"bigram_str\")) != \"\")\n",
    "\n",
    "#count most frequent bigrams\n",
    "large_count = cleaned_bigrams_str.groupBy(\"bigram_str\").agg(count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\"))\n",
    "\n",
    "\n",
    "#output\n",
    "large_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e2feb10-b769-4d8e-8220-3ac685960286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Further Investigation - most common trigrams\n",
    "\n",
    "#functions imported\n",
    "from pyspark.ml.feature import NGram, StopWordsRemover\n",
    "from pyspark.sql.functions import concat_ws, count, explode, col, lower\n",
    "\n",
    "#removal of punctuation\n",
    "large_no_punctuation = large.withColumn(\"sentence\", regexp_replace(\"sentence\", r\"[^\\w\\s]\", \"\"))\n",
    "\n",
    "#case normalisation\n",
    "large_lower = large_no_punctuation.withColumn(\"sentence_lower\", lower(col(\"sentence\")))\n",
    "\n",
    "#tokenisation\n",
    "large_array = large_lower.withColumn(\"sentence\", F.split(F.col(\"sentence_lower\"), \" \"))\n",
    "\n",
    "#removal of stopwords\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"sentence_filtered\")\n",
    "large_array_removed = remover.transform(large_array)\n",
    "\n",
    "#create the trigrams\n",
    "ngram = NGram(n=3, inputCol=\"sentence_filtered\", outputCol=\"trigrams\")\n",
    "large_trigrams = ngram.transform(large_array_removed)\n",
    "\n",
    "\n",
    "#make each trigram a row\n",
    "trigram_exploded = large_trigrams.select(explode(col(\"trigrams\")).alias(\"trigram\"))\n",
    "\n",
    "#convert the trigrams from an array to string to count\n",
    "trigrams_str = trigram_exploded.withColumn(\"trigram_str\", concat_ws(\" \", col(\"trigram\")))\n",
    "\n",
    "#remove empty rows\n",
    "clean_trigrams_str = trigrams_str.filter(F.trim(col(\"trigram_str\")) != \"\")\n",
    "\n",
    "#count most frequent bigrams\n",
    "large_count = clean_trigrams_str.groupBy(\"trigram_str\").agg(count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\"))\n",
    "\n",
    "\n",
    "#output\n",
    "large_count.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a048d436-df9b-4fe6-a95b-a99a60536d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 5 - Find out how many of the bigrams you’ve extracted from the Wikipedia subset appear in the list of idioms contained in the MAGPIE subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79456a46-cf9a-4361-a5ae-ecdb7affd7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read magpie dataset\n",
    "magpie = spark.read.json(\"dbfs:/FileStore/shared_uploads/bdavies5@sheffield.ac.uk/MAGPIE_unfiltered.jsonl\")\n",
    "\n",
    "#extract the idioms from magpie\n",
    "idioms = magpie.select('idiom')\n",
    "\n",
    "#join the idioms and wikipedia bigrams dataset\n",
    "idioms = idioms.withColumn('bigram_str', idioms['idiom'])\n",
    "join_matching = idioms.join(large_exploded_str, on=\"bigram_str\", how=\"inner\")\n",
    "\n",
    "#count the distinct matching values\n",
    "matching = join_matching.select(\"bigram_str\").distinct().count()\n",
    "print(f\"The number of wikipedia bigrams that are also idioms are: {matching}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76255c7-94bd-4a4d-858e-2688f17d67e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#further exploration, what are the bigrams that are also idioms, sorted by frequenxy\n",
    "\n",
    "#get frequency of each idiom\n",
    "idiom_frequency = join_matching.groupBy(\"bigram_str\").agg(F.count(\"*\").alias(\"frequency\"))\n",
    "\n",
    "\n",
    "#sort by descending frequency\n",
    "sorted_idioms = idiom_frequency.orderBy(F.desc(\"frequency\"), F.asc(\"bigram_str\"))\n",
    "\n",
    "#output\n",
    "display(sorted_idioms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b174b35c-1378-447d-9284-09d4a66c06b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#finding the genre of each idiom in the large bigrams dataset\n",
    "\n",
    "#select both idioms and bigrams from MAGPIE\n",
    "idiom_genre = magpie.select('idiom', 'genre'). withColumnRenamed('idiom', 'bigram_str')\n",
    "\n",
    "#join datasets\n",
    "joined = idiom_genre.join(large_exploded_str, on='bigram_str', how='inner')\n",
    "\n",
    "#get the frequency of each idiom\n",
    "idiom_count = joined.groupBy('bigram_str', 'genre').agg(F.count(\"*\").alias('frequency'))\n",
    "\n",
    "#sort by most frequent and top 10\n",
    "top10_idiom_genre = idiom_count.orderBy(F.desc(\"frequency\"), F.asc(\"bigram_str\")).limit(10)\n",
    "\n",
    "display(top10_idiom_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5867c9a1-70e7-450d-975c-b7d706091446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 6 - Ensuring that you are only considering the bigrams that appear in Wikipedia and not in MAGPIE, print out the 10 bigrams starting from rank 2500 when these are ordered by decreasing frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7feb79ab-0096-4e06-8acc-9eeaf413768c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#new dataset that only includes bigrams not found in the Magpie idiom dataset\n",
    "non_idiom_bigrams = large_exploded_str.join(idioms, on=\"bigram_str\", how=\"left_anti\")\n",
    "\n",
    "#get the frequency of these bigrams\n",
    "bigram_count = non_idiom_bigrams.groupBy(\"bigram_str\").agg(F.count(\"*\").alias(\"frequency\"))\n",
    "\n",
    "#rank the bigrams by frequency\n",
    "ranked_bigrams = bigram_count.orderBy(F.desc(\"frequency\"), F.asc(\"bigram_str\"))\n",
    "\n",
    "#output 10 bigrams starting from rank 2500\n",
    "top10 = ranked_bigrams.limit(2510).tail(10)\n",
    "\n",
    "for row in top10:\n",
    "    print(f\"{row['bigram_str']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e5d010-4754-4025-9955-610d4a0502ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how many unique bigrams are not in the magpie idoms dataset\n",
    "unique_wiki_bigrams_count = non_idiom_bigrams.select('bigram').distinct().count()\n",
    "\n",
    "print(f\"There are {unique_wiki_bigrams_count} bigrams not found in the MAGPIE idoms list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a0beb4-13d3-493b-8e89-62a9d7479855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DO the bigrams that share the same starting letter (The) have the same frequency\n",
    "#output 10 bigrams starting from rank 2500\n",
    "top10 = ranked_bigrams.limit(2510).tail(10)\n",
    "\n",
    "for row in top10:\n",
    "    print(f\"{row['bigram_str']} - {row['frequency']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b8985c-4d8f-4781-806a-e0fefc621a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "data cleanup techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ca6994-1854-4c9a-90d9-b5b9ee2c04e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data cleanup methods used during this report\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "\n",
    "#case normalisation\n",
    "large = large.withColumn(\"sentence_lower\", lower(col(\"sentence\")))\n",
    "\n",
    "#Removal of stopwords\n",
    "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"sentence_filtered\")\n",
    "large_stopwords_removed = remover.transform(large_array)\n",
    "\n",
    "#removal of punctuation\n",
    "large_no_punctuation = large.withColumn(\"sentence\", regexp_replace(\"sentence\", r\"[^\\w\\s]\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "INF6032 Report Questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}